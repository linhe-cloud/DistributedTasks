# 分布式任务平台从 0 到 1 构建步骤与思路

本文基于技术栈：Gin + Redis List + PostgreSQL + cron + Docker + Nginx + Supervisor，给出一个从 0 到 1 的落地步骤指南，帮助你按阶段推进实现。

## 阶段 0：初始化项目与基础环境

目标：把基础工程和依赖环境准备好，能跑起一个最简单的 Gin 服务。

步骤：
1. 初始化 Go 模块
   - 在项目根目录执行：`go mod init DistributedTasks`
   - 在 `main.go` 中创建一个最简单的 Gin HTTP 服务，暴露 `/healthz`。
2. 引入基础依赖
   - Gin Web 框架
   - PostgreSQL 驱动（`pgx` 或 `database/sql` + 驱动）
   - Redis 客户端（如 `go-redis`）
   - cron 库（如 `robfig/cron/v3`）
3. 配置管理
   - 定义基础配置结构：`AppConfig{ PostgresDSN, RedisAddr, HTTPPort, ... }`
   - 支持从环境变量加载（后续方便 Docker 部署）。
4. 本地环境准备
   - 本地安装或使用 docker-compose 起 `postgres`、`redis` 实例。
   - 使用简单账号密码，先确保能连接成功。

思路要点：
- 这一阶段只关注“能跑起来一个健康的 HTTP 服务 + 连上 DB/Redis”，不做业务逻辑。
- 所有配置统一从环境变量读取，为后面的容器化做准备。

---

## 阶段 1：定义任务模型与数据库表

目标：把“任务”长什么样（Task/TaskRun/Schedule/Worker）定下来，并在 PostgreSQL 中创建表结构。

步骤：
1. 先在代码中定义核心领域模型（结构体）：
   - `Task`：任务定义（id, name, type, priority, queue_name, payload, max_retries, retry_strategy, status, dedup_key, created_at...）
   - `TaskRun`：任务运行记录（id, task_id, attempt, status, worker_id, started_at, finished_at, result, next_retry_at...）
   - `Schedule`：定时计划（id, task_template_id, cron_expr, timezone, enabled, last_triggered_at）
   - `Worker`：Worker 元数据（id, name, queues, heartbeat_at, status, capacity）
2. 根据模型整理 DDL
   - 在 `技术文档/分布式任务平台技术方案.md` 的基础上，生成最终的建表 SQL。
   - 创建一个 `db/migrations` 目录存放 SQL 脚本（方便未来升级）。
3. 在本地数据库执行建表
   - 手动执行 SQL 或使用迁移工具（如 `golang-migrate`）应用迁移。
4. 在代码中封装数据访问层（Repository）
   - 定义接口，例如：`TaskRepository`、`TaskRunRepository`、`ScheduleRepository`、`WorkerRepository`。
   - 使用 `pgx` 或 `database/sql` 实现最基本的 CRUD（先不追求极致抽象）。

思路要点：
- 优先把“数据结构”想清楚，后续 API、队列消息都围绕这些模型展开。
- 表结构一旦上线后改动成本较高，前期多在文档里推演几轮。

---

## 阶段 2：实现最简 API 与任务创建流程（无队列）

目标：实现从 HTTP API 创建任务 → 写入 PostgreSQL → 查询任务 的闭环，先不进 Redis 队列。

步骤：
1. 设计 API 路由
   - POST `/api/v1/tasks`：创建任务（至少支持即时任务）。
   - GET `/api/v1/tasks/{id}`：根据 ID 查询任务。
2. 编写 Gin Handler
   - 入参结构体：包含 name, type, queue_name, payload, dedup_key, max_retries, retry_strategy 等。
   - 校验：必填字段、类型、长度、枚举值（immediate/scheduled）。
3. 在 Handler 中编排业务逻辑
   - 校验幂等（根据 dedup_key 查 tasks 表，若已存在且状态可复用则直接返回）。
   - 创建 Task 记录：设置初始 status（如 `pending` 或 `queued`）。
4. 实现查询任务接口
   - 根据 ID 查询 Task + 最近一次 TaskRun（如果有）。
5. 编写简单单元测试
   - 针对 Handler 做接口级测试（可使用内存 DB 或测试数据库）。

思路要点：
- 这一阶段只保证“HTTP → Postgres → HTTP”链路 OK，不引入 Redis 复杂度。
- 幂等逻辑提前设计好，避免后面加队列后到处重写。

---

## 阶段 3：接入 Redis 队列，实现即时任务入队与基础 Worker

目标：让即时任务真正进入 Redis 队列，并由 Worker 消费执行。

步骤：
1. 定义 Redis 队列规范
   - ready 队列（List）：`queue:{name}:ready`
   - delayed 队列（ZSET）：`queue:{name}:delayed`
   - dlq 队列（List）：`queue:{name}:dlq`
   - 载荷字段：task_run_id, task_id, attempt, payload, priority, lease_ttl。
2. 在创建任务流程中增加入队逻辑
   - 对即时任务：在事务中创建 Task + TaskRun 记录（attempt=1）。
   - 事务提交后，将构造的载荷 push 到 `queue:{queue_name}:ready`。
   - 更新 Task 状态为 `queued`。
3. 编写 Worker 程序（独立进程）
   - 使用 `go-redis` 连接 Redis。
   - 轮询 List：`BRPOP` 或 `BLPOP` 从多个队列拉取任务（支持 high/normal/low）。
   - 反序列化任务载荷，执行业务逻辑（此时可以先实现一个“打印日志 + 随机成功/失败”的 Demo）。
   - 成功时：更新 TaskRun 状态为 `succeeded`，Task 状态为 `completed`。
   - 失败时：调用重试策略，计算 next_retry_at，写入 delayed（ZSET），更新 TaskRun 为 `retrying`。
4. 验证闭环
   - 通过 API 创建任务 → 任务入队 → Worker 打日志执行 → DB 状态更新。

思路要点：
- 优先实现“简单消费 + 结果落库”，不急着上租约、心跳等高级机制。
- Worker 初版可以同步串行执行，后续再改成并发 pool。

---

## 阶段 4：引入重试、延时队列与 DLQ

目标：让失败任务按照策略自动重试，达到上限后进入死信队列。

步骤：
1. 实现延时队列搬运逻辑
   - 在 Worker 或单独的“延时队列搬运器”协程中：
     - 周期性扫描 `queue:{name}:delayed`（ZSET），取出 score <= 当前时间的元素。
     - 将其从 ZSET 移除并 push 回 `queue:{name}:ready`。
2. 完善重试策略
   - 在 Worker 中，失败时根据 retry_strategy 计算 next_retry_at：
     - 指数退避：`next = now + base * factor^attempt`，上限为 max。
   - 创建新的 TaskRun 记录，attempt+1，status=queued。
   - 达到 max_retries 时：
     - 将载荷 push 到 `queue:{name}:dlq`。
     - 将 Task 状态标记为 `failed`。
3. DLQ 管理
   - 暂时可以只提供一个简单的 API：
     - GET `/api/v1/queues/{name}/dlq`：查看死信任务。
     - POST `/api/v1/queues/{name}/dlq/replay`：重放指定任务回 ready。

思路要点：
- 重试逻辑一定要记录在 DB（TaskRun），方便排查。
- 搬运器可以初期做成 Worker 内部的 goroutine，后续再独立服务。

---

## 阶段 5：引入租约（Lease）与 Worker 心跳，提升可靠性

目标：解决 Worker 崩溃或长时间无响应导致任务“丢失/重复执行”的问题。

步骤：
1. 引入租约机制
   - 任务被 Worker 拉取后：
     - 设置 `lease:{task_run_id}` = worker_id，TTL=lease_ttl（如 30s）。
     - Worker 在执行过程中定期续租（如每 10s）。
   - 若 Worker 崩溃，租约到期：
     - 认为任务可被其他 Worker 接管（结合状态机判断）。
2. Worker 并发执行
   - 使用 goroutine pool（例如固定 worker_concurrency）：
     - 主循环负责从队列中取任务，放入本地 channel。
     - goroutine pool 并发消费 channel 中的任务。
3. Worker 元数据与心跳
   - 启动时注册 Worker：调用 API 或直接写 DB + Redis，记录订阅队列、capacity。
   - 周期性上报心跳（HTTP 或直接更新 DB），更新 workers 表的 heartbeat_at。
   - 提供 API 或后台任务标记长时间无心跳的 Worker 为 offline。

思路要点：
- 这一阶段你需要在“不过度复杂”和“足够可靠”之间平衡：
  - 可先只实现租约，不做“自动接管”，而是用于监控和人工处理。
  - 后续再增加“租约过期后重新入队”的自动化逻辑。

---

## 阶段 6：实现定时调度（Scheduler + cron）

目标：支持基于 cron 的定时任务调度，将“模板任务（task_template）”按时间生成具体 TaskRun 并入队。

步骤：
1. 定义定时任务创建流程
   - 在 API 中新增：
     - POST `/api/v1/schedules`：创建定时计划（task_template_id, cron_expr, timezone, enabled）。
   - TaskTemplate 可以复用 tasks 表中的一行，type=scheduled，payload 为模板参数。
2. 实现 Scheduler 服务
   - 独立进程（或与 API 同进程不同模块）：
     - 使用 `robfig/cron/v3` 注册多个 job，或自己基于 tick + cron 解析做调度。
   - 周期性读取 DB 中 enabled 的 schedules，根据 cron_expr & timezone 计算下一次触发时间。
   - 到点时：
     - 生成具体 Task（若需）与 TaskRun（attempt=1）。
     - 写入 DB 后入队（queue:ready）。
3. 补偿机制
   - 使用 `last_triggered_at`：
     - Scheduler 启动时检查是否有“过期未触发”的执行点。
     - 按策略决定是否补一轮（可配置是否自动补偿）。

思路要点：
- 刚开始可以不做“精确到秒”的调度，只要误差在数秒内即可。
- 把 cron 解析逻辑封装好，后续扩展时区、复杂表达式会简单很多。

---

## 阶段 7：接入 Nginx + Docker + Supervisor（部署落地）

目标：将系统容器化，并使用 Nginx 作网关、Supervisor 保活 Worker，支持多实例部署。

步骤：
1. 为各组件编写 Dockerfile
   - api、scheduler、worker 可共享基础镜像（Go 构建 → scratch/alpine runtime）。
   - 镜像中通过环境变量注入：DATABASE_URL、REDIS_URL、JWT_SECRET、QUEUE_NAMES、WORKER_CONCURRENCY 等。
2. 编写 docker-compose.yml（开发环境）
   - 包含：api、scheduler、worker、redis、postgres、nginx。
   - nginx.conf 里配置反向代理到 api，开启基础的超时与限流设置。
3. 在 worker 容器里接入 Supervisor
   - 使用 `supervisord` 管理多个 worker 进程：
     - 配置 `[program:worker]`，设置 auto-restart, redirect_stderr, stdout_logfile。
   - 验证 worker 崩溃自动重启。
4. 多实例与水平扩展
   - api、worker、scheduler 都可通过 scale 参数或 K8s 副本数扩容。
   - Redis+Postgres 先保持单实例；生产环境再考虑主从/托管。

思路要点：
- 开发环境优先使用 docker-compose 快速集成，后面再思考 K8s/Helm。
- 设计镜像和配置时，尽量保持“十二要素应用”风格，所有配置都由环境变量控制。

---

## 阶段 8：监控、日志与告警

目标：具备基础可观测性，方便排查问题和容量规划。

步骤：
1. 统一日志规范
   - 使用结构化日志（JSON），字段包含：timestamp, level, service, trace_id, task_id, run_id, worker_id, message。
   - 在 API 与 Worker 中使用统一的 logging 库。
2. 指标与监控
   - 指标（Prometheus 风格）：
     - 任务创建次数、执行成功/失败次数、重试次数。
     - 队列长度（ready/delayed/dlq）。
     - 任务延迟（创建到执行开始、执行耗时）。
     - Worker 并发利用率、心跳状态。
   - 暴露 `/metrics` 接口供 Prometheus 抓取（可作为后续增强）。
3. 告警
   - 阈值建议：
     - 某队列长度持续高于阈值。
     - 失败率或重试率异常升高。
     - 某 worker 心跳超时。
     - DLQ 增长过快。

思路要点：
- 监控不必一开始就做得很复杂，但要提前预留好指标和日志字段。
- 告警规则可先写在文档中，后续接入具体监控平台时直接搬过去。

---

## 阶段 9：安全与治理

目标：确保 API 使用安全、关键操作可审计。

步骤：
1. 接入认证与鉴权
   - 支持 API Key 或 JWT。
   - 针对管理操作（如重放 DLQ、修改 Schedule）做严格鉴权。
2. 参数与频率控制
   - Gin 中间件实现：
     - 全局/按 IP 的速率限制。
     - 请求大小限制（避免 payload 过大）。
3. 审计日志
   - 对任务创建/修改/取消/重放等操作写入审计表或审计日志。

思路要点：
- 初期可用简单 API Key + 中间件实现，后续视需要接入 OAuth2/RBAC 等复杂方案。

---

## 阶段 10：迭代与优化

目标：在系统可用的基础上逐步演进，支持更多场景与更高负载。

方向：
1. 性能优化
   - 优化 DB 查询（索引、批量更新）。
   - Worker 并发与连接池调优。
   - Redis 队列读写模式优化（Lua 脚本、pipeline）。
2. 功能增强
   - 多租户（按 namespace/tenant_id 隔离任务和队列）。
   - 任务编排（简单 DAG、依赖任务）。
   - Web 控制台（任务可视化、拖拽创建、重试/重放一键操作）。
3. 稳定性提升
   - 更细粒度的限流和熔断。
   - 不同业务线/队列的优先级策略与隔离。

---

## 总结：从 0 到 1 的推荐实施顺序

1. 搭脚手架：Go Modules + Gin 基础服务 + 配置 + DB/Redis 连接（阶段 0）。
2. 想清楚“任务模型”：定义 Task/TaskRun/Schedule/Worker 结构体和数据库表（阶段 1）。
3. 打通最小闭环：API 创建任务 → 写入 DB → 查询任务（阶段 2）。
4. 接入 Redis 队列：即时任务入队 + 基础 Worker 消费（阶段 3）。
5. 做好失败与重试：延时队列 + 重试策略 + DLQ（阶段 4）。
6. 提升可靠性：租约、心跳、并发 Worker（阶段 5）。
7. 支持定时任务：Scheduler + cron，定时生成 TaskRun 并入队（阶段 6）。
8. 落地部署：Docker 镜像、docker-compose、本地 Nginx + Supervisor（阶段 7）。
9. 增强可观测性：日志、指标、告警（阶段 8）。
10. 完善安全与治理，逐步优化性能与架构（阶段 9~10）。

通过以上步骤，你可以从一个“空仓库”逐步演进到一个可在生产环境使用的分布式任务平台。后续如果你希望，我可以针对某一阶段（比如“阶段 3：队列 + Worker”）直接给出 Gin 路由和 Go 代码骨架，帮助你开始实际编码。
